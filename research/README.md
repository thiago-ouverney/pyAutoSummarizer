# README â€” Pasta **`research/`**

> **PropÃ³sito**: Documentar o *workflow* de pesquisa desenvolvido para o TCC â€œAvaliaÃ§Ã£o de Modelos Abstrativos de SumarizaÃ§Ã£o: Uma Proposta de MÃ©trica Integrada â€“â€¯Lexical, SemÃ¢ntica e Factualâ€.
> Este README complementa o README raiz do projeto, descrevendo **exclusivamente** o processo exploratÃ³rio / experimental conduzido em *notebooks* e scripts localizados em `research/`.

---

## ğŸ“ Estrutura de diretÃ³rios

```
research/
â”œâ”€â”€ data/                  # artefatos (datasets, csv, jsonl, checkpointsâ€¦)
â”œâ”€â”€ data_processing/       # scripts auxiliares de preparaÃ§Ã£o de dados
â”‚   â””â”€â”€ pair_data.py
â”œâ”€â”€ src/                   # cÃ³digo Python reutilizÃ¡vel pelos notebooks
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ functions.py
â”‚   â””â”€â”€ model.py
â”œâ”€â”€ eda.ipynb
â”œâ”€â”€ join_data.ipynb
â”œâ”€â”€ evaluators.ipynb
â”œâ”€â”€ evaluator_questeval.ipynb
â”œâ”€â”€ optimization.ipynb
â”œâ”€â”€ summeval_metrics_notebook.ipynb
â””â”€â”€ <outros ipynbs>
```

* **`pyAutoSummarizer/`** (nÃ­vel raiz) contÃ©m as implementaÃ§Ãµes genÃ©ricas de avaliadores (lexical, semÃ¢ntico e factual) importadas pelos notebooks.
* **`requirements.txt`** e **`Makefile`** no repositÃ³rio raiz provÃªm o ambiente de execuÃ§Ã£o (ver *Setup* abaixo).

---

## âš™ï¸ Setup rÃ¡pido

```bash
# 1. Criar ambiente virtual (PythonÂ â‰¥â€¯3.9)
$ make create-venv      # ou python -m venv .venv

# 2. Instalar dependÃªncias e pacote em modo editÃ¡vel
$ make install-dependencies   # equiv. pip install -e .

# 3. (opcional) Ativar venv manualmente
$ source .venv/bin/activate   # Linux/macOS
$ .\.venv\Scripts\activate   # Windows
```

> **Dica**: Use a extensÃ£o *Jupyter* do VSâ€¯Code apontando para o interpretador de `./.venv` para executar os notebooks.

---

## ğŸ”„ Fluxo de trabalho dos notebooks

| Ordem | Notebook                              | Objetivo principal                                                                                                                             | Entradas                               | SaÃ­das/Artefatos                           |
| ----- | ------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------- | ------------------------------------------ |
| 1     | **`join_data.ipynb`**                 | Consolida o dataset **SummEval** (*model\_outputsÂ Ã—Â humanÂ annotations*) e grava `model_annotations.aligned.jsonl` em `data/`.                  | Raw jsonl do SummEval                  | `data/model_annotations.aligned.jsonl`     |
| 2     | **`eda.ipynb`**                       | AnÃ¡lise exploratÃ³ria: distribuiÃ§Ã£o de mÃ©tricas humanas, inspeÃ§Ã£o de outliers, verificaÃ§Ã£o de cobertura de modelos.                             | `data/model_annotations.aligned.jsonl` | grÃ¡ficos/insights (inline)                 |
| 3     | **`evaluators.ipynb`**                | Calcula mÃ©tricas **lexicais** (ROUGE, BLEU, METEOR), **semÃ¢nticas** (BERTScore, Sentence-BERT) e **factuais** (FactCC) via `pyAutoSummarizer`. | SaÃ­da do passoÂ 2                       | `data/df_agg.csv` (mÃ©tricas agregadas)     |
| 4     | **`evaluator_questeval.ipynb`**       | Benchmark extra com **QuestEval** para comparaÃ§Ã£o externa.                                                                                     | idem                                   | mÃ©tricas QuestEval (csv)                   |
| 5     | **`optimization.ipynb`**              | Otimiza pesos A<sub>i</sub>/B<sub>i</sub> da mÃ©trica hÃ­brida usando **Optuna** (`research/src/model.py`).                                      | `data/df_agg.csv` + mÃ©tricas humanas   | \_bestÂ *weights.json*, curva de otimizaÃ§Ã£o |
| 6     | **`summeval_metrics_notebook.ipynb`** | Gera tabelas finais de correlaÃ§Ã£o e validaÃ§Ã£o cruzada; compila resultados para o capÃ­tuloÂ *Resultados*.                                        | Pesos Ã³timos + datasets anteriores     | `data/avg_summeval_metrics.csv`, figuras   |

> **ExecuÃ§Ã£o incremental**: cada notebook verifica e reutiliza artefatos existentes; portanto, apÃ³s rodar o passoâ€¯1 uma Ãºnica vez, vocÃª pode reexecutar apenas os passos afetados por mudanÃ§as de cÃ³digo/dados.

---

## ğŸ“œ Scripts & mÃ³dulos auxiliares

### `data_processing/pair_data.py`

Recria os pares **(artigo, resumoÂ gerado)** a partir dos *storyÂ files* do CNN/DM quando necessÃ¡rio. Uso:

```bash
python research/data_processing/pair_data.py \
  --model_outputs ./data/model_outputs \
  --story_files   ./data/cnndm_stories
```

### `src/constants.py`

* Centraliza caminhos, listas de avaliadores (`LEXICAL_EVAL`, `SEMANTIC_EVAL`, `FACTUAL_EVAL`) e parÃ¢metros globais.

### `src/functions.py`

* FunÃ§Ãµes utilitÃ¡rias para:

  * agregaÃ§Ã£o de mÃ©tricas (`aggregate_dataframe`),
  * cÃ¡lculo de correlaÃ§Ãµes (`get_corr`, `get_corr_frame`),
  * construÃ§Ã£o da mÃ©trica hÃ­brida (`get_combinated_metric`),
  * cache de artefatos (`get_agg_frame`).

### `src/model.py`

* Define **`objective()`** (funÃ§Ã£oâ€‘alvo do Optuna) e **`get_best_weights()`** para extraÃ§Ã£o/normalizaÃ§Ã£o dos melhores pesos.

### `pyAutoSummarizer/base/evaluation/*`

* **`lexical.py`**: ROUGE, BLEU, METEOR
* **`semantic.py`**: BERTScore, Sentenceâ€‘BERT
* **`factual.py`**: FactCC
* Todas derivam da classeâ€‘base `EvaluationMetric` (`base.py`).

---

## â–¶ï¸ Como reproduzir o experimento completo

```bash
# Dentro da pasta research/ (ambiente jÃ¡ criado)

jupyter lab &   # ou code . para abrir no VSÂ Code

# 1. Execute sequencialmente os notebooks 1â†’6
# 2. Verifique em cada notebook a cÃ©lula "Config" â€“ ajuste paths se necessÃ¡rio
# 3. O resultado final (pesos + correlaÃ§Ãµes) serÃ¡ salvo em research/data/
```

> ğŸ“« DÃºvidas ou sugestÃµes? Abra uma *issue* ou contate **@thiagoâ€‘ouverney**.
