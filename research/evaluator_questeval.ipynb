{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb48aea",
   "metadata": {},
   "source": [
    "> Este notebook explica o racional de como instalar o quest-eval em um .venv dedicado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcc0cb",
   "metadata": {},
   "source": [
    "# Instala√ß√£o ‚Äúpip-only‚Äù do **QuestEval** (ambiente dedicado)\n",
    "\n",
    "Este guia replica o passo de `conda install pytorch cudatoolkit=‚Ä¶` da doc oficial, mas usando **venv + pip**.  \n",
    "Funciona em Windows, macOS ou Linux (troque apenas o comando de ativa√ß√£o do venv).\n",
    "\n",
    "---\n",
    "\n",
    "## Requisitos\n",
    "\n",
    "- **Python 3.9** (o spaCy 3.0 foi compilado para essa vers√£o).  \n",
    "- Acesso √† internet para baixar os wheels.\n",
    "\n",
    "---\n",
    "\n",
    "## 1  Criar e ativar o ambiente virtual\n",
    "\n",
    "<details>\n",
    "\n",
    "```bash\n",
    "# Windows (PowerShell)\n",
    "py -3.9 -m venv .venv-questeval\n",
    ".\\.venv-questeval\\Scripts\\activate\n",
    "\n",
    "# Linux / macOS (bash)\n",
    "python3.9 -m venv .venv-questeval\n",
    "source .venv-questeval/bin/activate\n",
    "````\n",
    "\n",
    "</details>\n",
    "\n",
    "> Atualize ferramentas b√°sicas:\n",
    "\n",
    "```bash\n",
    "python -m pip install -U pip setuptools wheel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2  Instalar **PyTorch** *antes de qualquer pacote*\n",
    "\n",
    "Escolha **uma** linha ‚Äî CPU ou GPU:\n",
    "\n",
    "```bash\n",
    "# CPU-only (roda em qualquer m√°quina)\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# GPU ‚Äì exemplo CUDA 11.8 (mude cu118 ‚Üí cu121, cu116‚Ä¶ se precisar)\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3  Instalar o conjunto de depend√™ncias compat√≠veis\n",
    "\n",
    "<details>\n",
    "\n",
    "```powershell\n",
    "# PowerShell ‚Äì use acento ` para quebrar linha\n",
    "pip install `\n",
    "  numpy==1.21.6 pandas==1.5.3 pyarrow==8.0.0 `\n",
    "  datasets==2.14.5 huggingface_hub==0.19.4 `\n",
    "  transformers==4.39.3 tokenizers==0.15.2 `\n",
    "  spacy==3.0.6 thinc==8.0.17 `\n",
    "  sentencepiece==0.1.95 bert_score==0.3.9 `\n",
    "  Unidecode==1.2.0 pytest==6.2.4\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Bash ‚Äì tudo em uma linha\n",
    "pip install \\\n",
    "  numpy==1.21.6 pandas==1.5.3 pyarrow==8.0.0 \\\n",
    "  datasets==2.14.5 huggingface_hub==0.19.4 \\\n",
    "  transformers==4.39.3 tokenizers==0.15.2 \\\n",
    "  spacy==3.0.6 thinc==8.0.17 \\\n",
    "  sentencepiece==0.1.95 bert_score==0.3.9 \\\n",
    "  Unidecode==1.2.0 pytest==6.2.4\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "**Por que essas vers√µes?**\n",
    "\n",
    "* `numpy 1.21.x` ‚Üí √∫nica s√©rie compat√≠vel com **spaCy 3.0 / Thinc 8.0**.\n",
    "* `huggingface_hub 0.19.4` ‚Üí j√° usa o endpoint novo (resolve *MissingSchema*) e ainda exp√µe `DatasetCard` que `datasets` precisa.\n",
    "* `transformers 4.39.3` ‚Üí aceita `hub 0.19.4` (dep ‚â• 0.14 < 1.0).\n",
    "* Demais pinos s√£o exigidos pelo QuestEval 0.2.4.\n",
    "\n",
    "---\n",
    "\n",
    "## 4  Baixar o modelo m√≠nimo do spaCy\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5  Instalar o QuestEval sem depend√™ncias\n",
    "\n",
    "```bash\n",
    "pip install --no-deps git+https://github.com/ThomasScialom/QuestEval@main\n",
    "```\n",
    "\n",
    "*(o `--no-deps` impede que ele derrube as vers√µes que voc√™ acabou de fixar).*\n",
    "\n",
    "---\n",
    "\n",
    "## 6  Teste r√°pido\n",
    "\n",
    "```python\n",
    "from questeval.questeval_metric import QuestEval\n",
    "\n",
    "qe = QuestEval(no_cuda=True)        # mude para False se quiser GPU\n",
    "print(qe.corpus_questeval(\n",
    "        hyp=[\"In 2002 Brazil became world champion.\"],\n",
    "        src=[\"Brazil won the 2002 World Cup.\"],\n",
    "        ref=[[\"Brazil won the 2002 FIFA World Cup in Japan.\"]]\n",
    "))\n",
    "```\n",
    "\n",
    "Sa√≠da esperada (exemplo):\n",
    "\n",
    "```python\n",
    "{'corpus_score': 0.62, 'ex_level_scores': [0.62]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7  (congelar) gerar especifica√ß√£o reprodut√≠vel\n",
    "\n",
    "```bash\n",
    "pip freeze > requirements-questeval.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Observa√ß√µes & Dicas\n",
    "\n",
    "* **N√£o** instale bibliotecas que exigem NumPy ‚â• 1.23 neste mesmo venv (ex.: matplotlib ‚â• 3.6).\n",
    "  Se precisar de gr√°ficos, use `matplotlib==3.5.3`.\n",
    "* Se usar GPU, troque o √≠ndice de PyTorch (`cu118`, `cu121`, etc.) para combinar com seu driver CUDA.\n",
    "* M√∫ltiplos projetos? Basta criar novos venvs e rodar\n",
    "  `pip install -r requirements-questeval.txt`.\n",
    "\n",
    "Com esses passos o QuestEval funciona sem recorrer ao Conda e sem colis√µes de depend√™ncia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3823ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv-questeval\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv-questeval\\lib\\site-packages\\questeval\\questeval_metric.py:106: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric_BERTScore = load_metric(\"bertscore\")\n",
      "c:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv-questeval\\lib\\site-packages\\torch\\__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_score': 0.852132914463679, 'ex_level_scores': [0.852132914463679]}\n"
     ]
    }
   ],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "qe = QuestEval(no_cuda=True)\n",
    "print(qe.corpus_questeval(\n",
    "        [\"In 2002, Brazil became world champion.\"],\n",
    "        [\"Brazil won the 2002 World Cup.\"],\n",
    "        [[\"Brazil won the 2002 FIFA World Cup in Japan.\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'src', 'text': 'Brazil won the 2002 World Cup.', 'self': {'NER': {'answers': ['Brazil', '2002', 'World Cup'], 'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['Which country won the 2002 World Cup?', 'In what year did Brazil win the World Cup?', 'What did Brazil win in 2002?']}}, 'NOUN': {'answers': ['Brazil', 'the 2002 World Cup'], 'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['Which country won the 2002 World Cup?', 'What did Brazil win?']}}}, 'asked': {'In what year did Brazil become world champion?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': '2002', 'answerability': 0.9897031784057617, 'ground_truth': {'2002': {'bertscore': 1.0, 'f1': 1.0}}}}, 'What country became the world champion in 2002?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'Brazil', 'answerability': 0.9688682556152344, 'ground_truth': {'Brazil': {'bertscore': 1.0000003576278687, 'f1': 1.0}}}}, 'What title did Brazil win in 2002?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'World Cup', 'answerability': 0.8806015253067017, 'ground_truth': {'world champion': {'bertscore': 0.7933896780014038, 'f1': 0.5}}}}}}\n"
     ]
    }
   ],
   "source": [
    "log = qe.open_log_from_text(\"Brazil won the 2002 World Cup.\")\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv-questeval\\lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_score': 0.6115364335096283, 'ex_level_scores': [0.5698804503395444, 0.6531924166797121]}\n"
     ]
    }
   ],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "questeval = QuestEval()\n",
    "\n",
    "source_1 = \"Since 2000, the recipient of the Kate Greenaway medal has also been presented with the Colin Mears award to the value of 35000.\"\n",
    "prediction_1 = \"Since 2000, the winner of the Kate Greenaway medal has also been given to the Colin Mears award of the Kate Greenaway medal.\"\n",
    "references_1 = [\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal will also receive the Colin Mears Awad which worth 5000 pounds\",\n",
    "    \"Since 2000, the recipient of the Kate Greenaway Medal has also been given the Colin Mears Award.\"\n",
    "]\n",
    "\n",
    "source_2 = \"He is also a member of another Jungiery boyband 183 Club.\"\n",
    "prediction_2 = \"He also has another Jungiery Boyband 183 club.\"\n",
    "references_2 = [\n",
    "    \"He's also a member of another Jungiery boyband, 183 Club.\", \n",
    "    \"He belonged to the Jungiery boyband 183 Club.\"\n",
    "]\n",
    "\n",
    "score = questeval.corpus_questeval(\n",
    "    hypothesis=[prediction_1, prediction_2], \n",
    "    sources=[source_1, source_2],\n",
    "    list_references=[references_1, references_2]\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e01ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'src', 'text': 'Since 2000, the recipient of the Kate Greenaway medal has also been presented with the Colin Mears award to the value of 35000.', 'self': {'NER': {'answers': ['2000', 'the Kate Greenaway', '35000'], 'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['When was the Kate Greenaway medal awarded?', 'What medal has been presented to the recipient of the Colin Mears award since 2000?', 'How much is the Colin Mears award worth?']}}, 'NOUN': {'answers': ['the recipient', 'the Kate Greenaway medal', 'the Colin Mears award', 'the value'], 'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['Who is the Kate Greenaway medal?', \"What award has been presented to the recipient of the Queen's award since 2000?\", 'What award has been presented to the recipient of the Kate Greenaway medal since 2000?', 'How much is the Colin Mears award?']}}}, 'asked': {'Since what year has the winner of the Kate Greenaway medal been given?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': '2000', 'answerability': 0.994324803352356, 'ground_truth': {'2000': {'bertscore': 1.0000001192092896, 'f1': 1.0}}}}, 'What medal has been given to the winner of the Colin Mears award since 2000?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'unanswerable', 'answerability': 0.42623037099838257, 'ground_truth': {'the Kate Greenaway': {'bertscore': 0.6808881759643555, 'f1': 0}}}}, 'What has been given to the Colin Mears award since 2000?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'unanswerable', 'answerability': 0.007761120796203613, 'ground_truth': {'the winner': {'bertscore': 0.7037416696548462, 'f1': 0}}}}, 'What award has been given to the winner of the Colin Mears award?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'unanswerable', 'answerability': 0.014314413070678711, 'ground_truth': {'the Kate Greenaway medal': {'bertscore': 0.6200703382492065, 'f1': 0}}}}, 'What award has been given to the winner of the Kate Greenaway medal since 2000?': {'QA_hash=ThomasNLG/t5-qa_squad2neg-en': {'answer': 'Colin Mears award', 'answerability': 0.9855555891990662, 'ground_truth': {'the Colin Mears award': {'bertscore': 0.9263917207717896, 'f1': 1.0}}}}}}\n"
     ]
    }
   ],
   "source": [
    "log = questeval.open_log_from_text(source_1)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18749bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type', 'text', 'self', 'asked']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(log.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dad8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NER': {'answers': ['2000', 'the Kate Greenaway', '35000'],\n",
       "  'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['When was the Kate Greenaway medal awarded?',\n",
       "    'What medal has been presented to the recipient of the Colin Mears award since 2000?',\n",
       "    'How much is the Colin Mears award worth?']}},\n",
       " 'NOUN': {'answers': ['the recipient',\n",
       "   'the Kate Greenaway medal',\n",
       "   'the Colin Mears award',\n",
       "   'the value'],\n",
       "  'QG_hash=ThomasNLG/t5-qg_squad1-en': {'questions': ['Who is the Kate Greenaway medal?',\n",
       "    \"What award has been presented to the recipient of the Queen's award since 2000?\",\n",
       "    'What award has been presented to the recipient of the Kate Greenaway medal since 2000?',\n",
       "    'How much is the Colin Mears award?']}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log[\"self\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\thiago.ouverney\\\\Projetos\\\\pyAutoSummarizer\\\\data\\\\model_annotations_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee7e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            1\n",
       "id                     dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2\n",
       "decoded               paul merson has restarted his row with andros ...\n",
       "expert_annotations    [{'coherence': 3, 'consistency': 5, 'fluency':...\n",
       "turker_annotations    [{'coherence': 2, 'consistency': 3, 'fluency':...\n",
       "references            [\"Andros Townsend an 83rd minute sub in Totten...\n",
       "model_id                                                            M13\n",
       "filepath              cnndm/dailymail/stories/8764fb95bfad8ee8492748...\n",
       "story_id                       8764fb95bfad8ee849274873a92fb8d6b400eee2\n",
       "existe_story                                                       True\n",
       "content               Paul Merson has restarted his row with Andros ...\n",
       "Name: 1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73169f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from questeval.questeval_metric import QuestEval\n",
    "qe = QuestEval(no_cuda=True)\n",
    "print(qe.corpus_questeval(\n",
    "        hypothesis=[df.loc[1,\"decoded\"]],\n",
    "        sources=[df.loc[1,\"content\"]],\n",
    "        list_references=[eval(df.loc[1,\"references\"])[0]]\n",
    "        )\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-feqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
