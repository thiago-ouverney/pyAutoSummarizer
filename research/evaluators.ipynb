{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106e9659",
   "metadata": {},
   "source": [
    "# Evaluators Validation Notebook \n",
    "Este notebook demonstra, **c√°lculo a c√°lculo**, como devem ser obtidas as\n",
    "pontua√ß√µes de cada m√©trica usada em *pyAutoSummarizer* **sem** importar as\n",
    "classes prontas de `evaluation/`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c67911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Depend√™ncias essenciais\n",
    "# !pip -q install rouge-score nltk sentence-transformers gensim\n",
    "import warnings, nltk\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "nltk.download('punkt_tab', quiet=True) # Baixa o PunktTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390317ba",
   "metadata": {},
   "source": [
    "## 1‚ÄØ¬†Lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da1ba1",
   "metadata": {},
   "source": [
    "> Precisaremos aplicar ``StopWords`` + ``Stemming`` antes de aplicar cada Evaluator Lexical\n",
    "\n",
    "> Nele, precisaremos flaguear se as aplica√ß√µes ser√£o em ``PT`` ou ``EN``, √∫nicas transforma√ß√µes dispon√≠veis inicialmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3994d32",
   "metadata": {},
   "source": [
    "**Exemplo**\n",
    "- Refer√™ncia: ‚ÄúEu amo estudar na Universidade Federal Fluminense.‚Äù  \n",
    "- Resumo gerado: ‚ÄúEu adoro estudar na UFF.‚Äù\n",
    "\n",
    "Calcularemos tr√™s m√©tricas tradicionais:\n",
    "1. ROUGE‚Äë1 F1  \n",
    "2. ROUGE‚ÄëL F1  \n",
    "3. BLEU‚Äë4  \n",
    "4. METEOR\n",
    "\n",
    "üìù **Formato esperado** ‚Äî um `dict` com quatro chaves (`rouge1_f1`,\n",
    "`rougeL_f1`, `bleu4`, `meteor`) e valores `float ‚àà [0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d26b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_f1': 0.5, 'rougeL_f1': 0.5, 'bleu4': 0.08428828344718171, 'meteor': 0.3758169934640523}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "reference = \"Eu amo estudar na Universidade Federal Fluminense.\"\n",
    "generated  = \"Eu adoro estudar na UFF.\"\n",
    "\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True) \n",
    "# Aqui √© aplicado um stemmer em ingl√™s, \n",
    "# Para aplicarmos nosso caso de texto ptbr, preciar√≠amos usar o RSLPSTemmer \n",
    "rouge_scores = scorer.score(reference, generated)\n",
    "lexical = {\n",
    "    'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
    "    'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
    "}\n",
    "\n",
    "# BLEU‚Äë4\n",
    "chen = SmoothingFunction()\n",
    "lexical['bleu4'] = sentence_bleu(\n",
    "    [reference.split()], # Token de Reference \n",
    "    generated.split(), # Token de Generado\n",
    "    weights=(0.25,0.25,0.25,0.25), # Pesos para BLEU-4 (4 pesos pois Bleu4)\n",
    "    smoothing_function=chen.method1 # Evita que o produto vire 0\n",
    ")\n",
    "\n",
    "# METEOR\n",
    "lexical['meteor'] = meteor_score([reference.split()], generated.split())\n",
    "\n",
    "print(lexical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0626d90",
   "metadata": {},
   "source": [
    "### Tratando Dados para uma Avalia√ß√£o Correta (PTBR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061c1a",
   "metadata": {},
   "source": [
    "> Precisamos remover ``stopwords`` e ``stemming`` para ter uma avalia√ß√£o mais fiel a realidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer, SnowballStemmer\n",
    "nltk.download('rslp',quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text: str,LANGUAGE='pt') -> str:\n",
    "    if LANGUAGE == \"pt\":\n",
    "        _STOP_SET  = set(stopwords.words(\"portuguese\"))\n",
    "        _STEMMER   = RSLPStemmer()\n",
    "        _TOKEN_LANG = \"portuguese\"\n",
    "    else:                          # english\n",
    "        _STOP_SET  = set(stopwords.words(\"english\"))\n",
    "        # Porter √© o mais leve; Snowball costuma ser ligeiramente melhor\n",
    "        _STEMMER   = SnowballStemmer(\"english\")\n",
    "        _TOKEN_LANG = \"english\"\n",
    "    \n",
    "\n",
    "    def stemming(generated):\n",
    "        tokens = [_STEMMER.stem(w) for w in generated.split()]\n",
    "        texto_stemmed = \" \".join(tokens)\n",
    "        return texto_stemmed\n",
    "\n",
    "    def remove_stopwords(text: str) -> str:\n",
    "        return \" \".join(\n",
    "            w for w in word_tokenize(text, language=_TOKEN_LANG)\n",
    "            if w.lower() not in _STOP_SET\n",
    "        )\n",
    "\n",
    "    def remove_ruido(texto: str) -> str:\n",
    "        _URLS_HASHTAGS_MENCOES = re.compile(r'https?://\\S+|www\\.\\S+|[@#]\\w+', flags=re.UNICODE)\n",
    "        _DIGITOS                = re.compile(r'\\d+', flags=re.UNICODE)\n",
    "        _PONTUACAO_E_SIMBOLOS   = re.compile(r'[^A-Za-z√Ä-√ñ√ò-√∂√∏-√ø\\s]', flags=re.UNICODE)\n",
    "        _ESPACOS                = re.compile(r'\\s+', flags=re.UNICODE)\n",
    "\n",
    "        # 1) URLs, hashtags, @men√ß√µes\n",
    "        texto = _URLS_HASHTAGS_MENCOES.sub(' ', texto)\n",
    "\n",
    "        # 2) D√≠gitos\n",
    "        texto = _DIGITOS.sub(' ', texto)\n",
    "\n",
    "        # 3) Pontua√ß√£o e s√≠mbolos \n",
    "        texto = _PONTUACAO_E_SIMBOLOS.sub(' ', texto)\n",
    "\n",
    "        # 4) Espa√ßos m√∫ltiplos\n",
    "        texto = _ESPACOS.sub(' ', texto).strip()\n",
    "\n",
    "        # 7. Caixa baixa (consist√™ncia)\n",
    "        return texto.lower()\n",
    "\n",
    "    text = remove_ruido(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "453c141f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quick brown fox jump lazi dog'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"A quick brown fox jumps over the lazy dog.\"\n",
    "generated  = \"A fast brown fox leaps over a lazy dog.\"\n",
    "preprocess_text(reference,LANGUAGE='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef089e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"Eu amo estudar na Universidade Federal Fluminense.\"\n",
    "generated  = \"Eu adoro estudar na UFF.\"\n",
    "reference_preprocessed = preprocess_text(reference)\n",
    "generated_preprocessed  = preprocess_text(generated)\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True) \n",
    "# Aqui √© aplicado um stemmer em ingl√™s, \n",
    "# Para aplicarmos nosso caso de texto ptbr, preciar√≠amos usar o RSLPSTemmer \n",
    "rouge_scores = scorer.score(reference_preprocessed, generated_preprocessed)\n",
    "lexical = {\n",
    "    'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
    "    'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
    "}\n",
    "\n",
    "# BLEU‚Äë4\n",
    "chen = SmoothingFunction()\n",
    "lexical['bleu4'] = sentence_bleu(\n",
    "    [reference_preprocessed.split()], # Token de Reference \n",
    "    generated_preprocessed.split(), # Token de Generado\n",
    "    weights=(0.25,0.25,0.25,0.25), # Pesos para BLEU-4 (4 pesos pois Bleu4)\n",
    "    smoothing_function=chen.method1 # Evita que o produto vire 0\n",
    ")\n",
    "\n",
    "# METEOR\n",
    "lexical['meteor'] = meteor_score([reference_preprocessed.split()], generated_preprocessed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95cb6857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amo estud univers feder flumin ador estud uff {'rouge1_f1': 0.25, 'rougeL_f1': 0.25, 'bleu4': 0.05833544737207805, 'meteor': 0.10416666666666666}\n"
     ]
    }
   ],
   "source": [
    "print(reference_preprocessed,generated_preprocessed,lexical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb3591",
   "metadata": {},
   "source": [
    "## 2‚ÄØ¬†Semantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93e8db",
   "metadata": {},
   "source": [
    "Agora mediremos similaridade de significado usando **Sentence‚ÄëBERT** . Escolhemos uma frase de exemplo em ingl√™s\n",
    "porque os modelos pr√©‚Äëtreinados s√£o mais robustos nesse idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a9f86",
   "metadata": {},
   "source": [
    "### Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a8a71",
   "metadata": {},
   "source": [
    "> Faz sentido aplicar para par√°grafos completos ? Deveria quebrar por senten√ßas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2473b",
   "metadata": {},
   "source": [
    "* Podemos gerar um embedding represetnando as sente√ßas e comparar entre si\n",
    "* Podemos considerar tudo um s√≥ senten√ßa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cb8fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome\n",
      " - The dog plays in the garden   : 0.0543\n",
      " - The new movie is so great     : 0.8939\n",
      " - A woman watches TV            : -0.0502\n",
      "The cat sits outside\n",
      " - The dog plays in the garden   : 0.2838\n",
      " - The new movie is so great     : -0.0029\n",
      " - A woman watches TV            : 0.1310\n",
      "A man is playing guitar\n",
      " - The dog plays in the garden   : 0.2277\n",
      " - The new movie is so great     : -0.0136\n",
      " - A woman watches TV            : -0.0327\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    \"The new movie is awesome\",\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"The dog plays in the garden\",\n",
    "    \"The new movie is so great\",\n",
    "    \"A woman watches TV\",\n",
    "]\n",
    "\n",
    "# Compute embeddings for both lists\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = model.similarity(embeddings1, embeddings2)\n",
    "\n",
    "# Output the pairs with their score\n",
    "for idx_i, sentence1 in enumerate(sentences1):\n",
    "    print(sentence1)\n",
    "    for idx_j, sentence2 in enumerate(sentences2):\n",
    "        print(f\" - {sentence2: <30}: {similarities[idx_i][idx_j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ae612",
   "metadata": {},
   "source": [
    "### BERT Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385b592",
   "metadata": {},
   "source": [
    "**Representa√ß√£o de Tokens:** \n",
    "    O BERTScore utiliza embeddings contextuais (provenientes de modelos pr√©-treinados como BERT, RoBERTa, XLNet ou XLM) para representar os tokens nas senten√ßas de entrada. A vantagem desses embeddings √© que eles geram representa√ß√µes vetoriais diferentes para a mesma palavra em senten√ßas distintas, dependendo do contexto circundante.\n",
    "\n",
    "**Medida de Similaridade:** \n",
    "    A similaridade entre um token $x_i$ da refer√™ncia e um token $\\hat{x}_j$ da candidata √© calculada pela similaridade de cosseno entre seus respectivos vetores de embedding. Essa abordagem oferece uma medida de similaridade \"suave\", superando a necessidade de correspond√™ncia exata de strings.\n",
    "\n",
    "**Correspond√™ncia Greedy:** Para calcular a precis√£o e o recall, o BERTScore emprega uma estrat√©gia de correspond√™ncia gulosa (greedy matching). Isso significa que cada token √© correspondido ao token mais similar na outra senten√ßa para maximizar a pontua√ß√£o de similaridade.\n",
    "\n",
    "**C√°lculo das Pontua√ß√µes (Precision, Recall, F1):**\n",
    "\n",
    "> Recall (R_BERT): √â a m√©dia das similaridades de cosseno m√°ximas, onde cada token da refer√™ncia √© correspondido ao token mais similar na candidata \n",
    "\n",
    "> Precision (P_BERT): √â a m√©dia das similaridades de cosseno m√°ximas, onde cada token da candidata √© correspondido ao token mais similar na refer√™ncia.\n",
    "\n",
    "> F1 (F_BERT): √â a m√©dia harm√¥nica da Precis√£o e do Recall. De forma geral, a recomenda√ß√£o √© usar F1, pois ele se mostra mais confi√°vel em diversos cen√°rios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7df2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import bert_score\n",
    "reference = \"A quick brown fox jumps over the lazy dog.\"\n",
    "generated  = \"A fast brown fox leaps over a lazy dog.\"\n",
    "lang = 'en'  # Define the language for BERTScore\n",
    "P, R, F1 = bert_score.score([generated], [reference], lang=lang,\n",
    "                             rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40ee97",
   "metadata": {},
   "source": [
    "https://github.com/Tiiiger/bert_score#readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e376c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8388]) tensor([0.8388]) tensor([0.8391])\n"
     ]
    }
   ],
   "source": [
    "print(P,R,F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7b8d5",
   "metadata": {},
   "source": [
    "## 3‚ÄØ¬†Factual  \n",
    "*(m√©tricas ser√£o adicionadas futuramente)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
