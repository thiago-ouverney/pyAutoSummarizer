{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1b2358",
   "metadata": {},
   "source": [
    "# SummEval – Exploração e Estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2dcb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "DATA_PATH = Path(\"C:\\\\Users\\\\thiago.ouverney\\\\Projetos\\\\pyAutoSummarizer\\\\data\\\\model_annotations.aligned.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03edaeb",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos dados\n",
    "Função auxiliar para ler `.jsonl` e criar um `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38730d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>expert_coherence</th>\n",
       "      <th>turker_coherence</th>\n",
       "      <th>expert_consistency</th>\n",
       "      <th>turker_consistency</th>\n",
       "      <th>expert_fluency</th>\n",
       "      <th>turker_fluency</th>\n",
       "      <th>expert_relevance</th>\n",
       "      <th>turker_relevance</th>\n",
       "      <th>summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2</td>\n",
       "      <td>M11</td>\n",
       "      <td>paul merson was brought on with only seven min...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2</td>\n",
       "      <td>M13</td>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2</td>\n",
       "      <td>M1</td>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.6</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2</td>\n",
       "      <td>M14</td>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2</td>\n",
       "      <td>M15</td>\n",
       "      <td>paul merson has restarted his row with andros ...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             doc_id model_id  \\\n",
       "0  dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2      M11   \n",
       "1  dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2      M13   \n",
       "2  dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2       M1   \n",
       "3  dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2      M14   \n",
       "4  dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2      M15   \n",
       "\n",
       "                                             summary  expert_coherence  \\\n",
       "0  paul merson was brought on with only seven min...          1.333333   \n",
       "1  paul merson has restarted his row with andros ...          2.333333   \n",
       "2  paul merson has restarted his row with andros ...          2.333333   \n",
       "3  paul merson has restarted his row with andros ...          1.666667   \n",
       "4  paul merson has restarted his row with andros ...          3.333333   \n",
       "\n",
       "   turker_coherence  expert_consistency  turker_consistency  expert_fluency  \\\n",
       "0               3.0                 1.0                 3.0        3.000000   \n",
       "1               2.0                 5.0                 3.0        5.000000   \n",
       "2               3.8                 5.0                 4.2        5.000000   \n",
       "3               5.0                 5.0                 5.0        5.000000   \n",
       "4               2.0                 5.0                 4.0        3.333333   \n",
       "\n",
       "   turker_fluency  expert_relevance  turker_relevance  summary_len  \n",
       "0             4.0          1.666667               3.0           65  \n",
       "1             2.0          2.666667               3.0           46  \n",
       "2             3.8          2.666667               4.6           63  \n",
       "3             5.0          2.666667               4.0           47  \n",
       "4             2.0          4.000000               4.0           81  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_summ_eval(jsonl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Lê arquivo SummEval (.jsonl) e devolve DataFrame tabular.\"\"\"\n",
    "    records = []\n",
    "    with jsonl_path.open(encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            data = json.loads(line)\n",
    "            rec = {\n",
    "                \"doc_id\": data[\"id\"],\n",
    "                \"model_id\": data[\"model_id\"],\n",
    "                \"summary\": data[\"decoded\"],\n",
    "            }\n",
    "            # Médias de especialistas e turkers\n",
    "            for metric in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]:\n",
    "                rec[f\"expert_{metric}\"] = np.mean([ann[metric] for ann in data[\"expert_annotations\"]])\n",
    "                rec[f\"turker_{metric}\"] = np.mean([ann[metric] for ann in data[\"turker_annotations\"]])\n",
    "            records.append(rec)\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"summary_len\"] = df[\"summary\"].str.split().apply(len)\n",
    "    return df\n",
    "\n",
    "df = load_summ_eval(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa39cc7",
   "metadata": {},
   "source": [
    "## 2. Estatísticas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a01176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_doc</th>\n",
       "      <th>N_sys</th>\n",
       "      <th>L_sum_mean</th>\n",
       "      <th>L_sum_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>62.99</td>\n",
       "      <td>19.800914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_doc  N_sys  L_sum_mean  L_sum_std\n",
       "0    100     16       62.99  19.800914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_basic_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"N_doc\": [df[\"doc_id\"].nunique()],\n",
    "        \"N_sys\": [df[\"model_id\"].nunique()],\n",
    "        \"L_sum_mean\": [df[\"summary_len\"].mean()],\n",
    "        \"L_sum_std\": [df[\"summary_len\"].std()],\n",
    "    })\n",
    "\n",
    "basic_stats = compute_basic_stats(df)\n",
    "basic_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d1580",
   "metadata": {},
   "source": [
    "## 3. Médias das notas (especialistas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394de890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert_coherence      3.412500\n",
       "expert_consistency    4.660417\n",
       "expert_fluency        4.672917\n",
       "expert_relevance      3.777083\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_scores(df: pd.DataFrame, prefix: str) -> pd.Series:\n",
    "    cols = [f\"{prefix}_{m}\" for m in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]]\n",
    "    return df[cols].mean()\n",
    "\n",
    "expert_means = mean_scores(df, \"expert\")\n",
    "expert_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50212ad",
   "metadata": {},
   "source": [
    "## 4. Correlação Spearman – Expert × Turker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ab35a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coherence      0.039070\n",
       "consistency   -0.010086\n",
       "fluency        0.050965\n",
       "relevance      0.090638\n",
       "mean           0.042647\n",
       "Name: rho_E_T, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spearman_expert_turker(df: pd.DataFrame) -> pd.Series:\n",
    "    rho = {}\n",
    "    for m in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]:\n",
    "        rho[m], _ = spearmanr(df[f\"expert_{m}\"], df[f\"turker_{m}\"])\n",
    "    rho[\"mean\"] = np.mean(list(rho.values()))\n",
    "    return pd.Series(rho, name=\"rho_E_T\")\n",
    "\n",
    "rho_e_t = spearman_expert_turker(df)\n",
    "rho_e_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8858016",
   "metadata": {},
   "source": [
    "## 5. Correlação média entre Turkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c3a20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49577505280708467"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_turker_pairwise_corr(jsonl_path: Path) -> float:\n",
    "    import collections\n",
    "    turker_data = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    with jsonl_path.open(encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            data = json.loads(line)\n",
    "            annots = data[\"turker_annotations\"]\n",
    "            if len(annots) < 2:\n",
    "                continue\n",
    "            for idx, ann in enumerate(annots):\n",
    "                for m in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]:\n",
    "                    turker_data[idx][m].append(ann[m])\n",
    "    pairwise = []\n",
    "    for m in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]:\n",
    "        keys = list(turker_data.keys())\n",
    "        for i, a in enumerate(keys):\n",
    "            for b in keys[i+1:]:\n",
    "                rho, _ = spearmanr(turker_data[a][m], turker_data[b][m])\n",
    "                pairwise.append(rho)\n",
    "    return float(np.mean(pairwise))\n",
    "\n",
    "rho_T = mean_turker_pairwise_corr(DATA_PATH)\n",
    "rho_T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4054322",
   "metadata": {},
   "source": [
    "## 6. Sumário consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7febe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'coherence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'coherence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m      1\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescrição\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero total de artigos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero total de resumos de modelos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComprimento médio do resumo (tokens)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDesvio-padrão do comprimento de resumo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMédia das notas de coerência (especialistas)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMédia das notas de consistência\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMédia das notas de fluência\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMédia das notas de relevância\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelação Spearman média Expert × Turker\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelação Spearman média entre Turkers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     ],\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSímbolo\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_sys\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¯L_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mσ_\u001b[39m\u001b[38;5;132;01m{L_sum}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mμ_coh^\u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mμ_cons^\u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mμ_flu^\u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mμ_rel^\u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mρ_\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mE×T}\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mρ_T\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     ],\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mint\u001b[39m(basic_stats\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28mint\u001b[39m(basic_stats\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_sys\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mround\u001b[39m(basic_stats\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_sum_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mround\u001b[39m(basic_stats\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_sum_std\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28mround\u001b[39m(\u001b[43mexpert_means\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoherence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mround\u001b[39m(expert_means[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsistency\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mround\u001b[39m(expert_means[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfluency\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mround\u001b[39m(expert_means[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mround\u001b[39m(rho_e_t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mround\u001b[39m(rho_T, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     30\u001b[0m     ]\n\u001b[0;32m     31\u001b[0m })\n\u001b[0;32m     32\u001b[0m summary_df\n",
      "File \u001b[1;32mc:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\thiago.ouverney\\Projetos\\pyAutoSummarizer\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'coherence'"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    \"Descrição\": [\n",
    "        \"Número total de artigos\",\n",
    "        \"Número total de resumos de modelos\",\n",
    "        \"Comprimento médio do resumo (tokens)\",\n",
    "        \"Desvio-padrão do comprimento de resumo\",\n",
    "        \"Média das notas de coerência (especialistas)\",\n",
    "        \"Média das notas de consistência\",\n",
    "        \"Média das notas de fluência\",\n",
    "        \"Média das notas de relevância\",\n",
    "        \"Correlação Spearman média Expert × Turker\",\n",
    "        \"Correlação Spearman média entre Turkers\",\n",
    "    ],\n",
    "    \"Símbolo\": [\n",
    "        \"N_doc\", \"N_sys\", \"¯L_sum\", \"σ_{L_sum}\",\n",
    "        \"μ_coh^{exp}\", \"μ_cons^{exp}\", \"μ_flu^{exp}\", \"μ_rel^{exp}\",\n",
    "        \"ρ_{E×T}\", \"ρ_T\"\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        int(basic_stats.at[0, \"N_doc\"]),\n",
    "        int(basic_stats.at[0, \"N_sys\"]),\n",
    "        round(basic_stats.at[0, \"L_sum_mean\"], 2),\n",
    "        round(basic_stats.at[0, \"L_sum_std\"], 2),\n",
    "        round(expert_means[\"coherence\"], 2),\n",
    "        round(expert_means[\"consistency\"], 2),\n",
    "        round(expert_means[\"fluency\"], 2),\n",
    "        round(expert_means[\"relevance\"], 2),\n",
    "        round(rho_e_t[\"mean\"], 3),\n",
    "        round(rho_T, 3)\n",
    "    ]\n",
    "})\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a54961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcff5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08db99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
